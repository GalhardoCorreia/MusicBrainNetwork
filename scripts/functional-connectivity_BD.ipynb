{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nilearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from nilearn import plotting\n",
    "from nilearn import connectome\n",
    "from nilearn import datasets\n",
    "from nilearn import image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the data directory\n",
    "src_dir = '../data'\n",
    "\n",
    "# Set the participant and session IDs\n",
    "part_id = 'sub-01'\n",
    "ses_id = 'ses-01'\n",
    "\n",
    "# Path to the T1w image\n",
    "anat_path = os.path.join(src_dir, part_id, ses_id, 'anat', f'{part_id}_{ses_id}_space-MNI152NLin2009cAsym_res-2_desc-preproc_T1w.nii.gz')\n",
    "\n",
    "# Path to functional image\n",
    "funcdir_path = os.path.join(src_dir, part_id, ses_id, 'func')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Anat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.plotting import plot_anat\n",
    "\n",
    "plot_anat(anat_path, title='T1w image', display_mode='ortho',  draw_cross=False,  cut_coords=(0, 0, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the functional data\n",
    "\n",
    "# list directory contents with extension 'nii.gz'\n",
    "func_runs = os.listdir(funcdir_path)\n",
    "func_runs = [f for f in func_runs if f.endswith('bold.nii.gz')]\n",
    "\n",
    "# Sort the list\n",
    "func_runs.sort()\n",
    "\n",
    "print(func_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import mean_img , load_img\n",
    "from nilearn.plotting import plot_img\n",
    "\n",
    "# Load the first functional image\n",
    "func_image = image.load_img(os.path.join(funcdir_path,func_runs[0]))\n",
    "\n",
    "mfunc_img = mean_img(func_image)\n",
    "\n",
    "plot_img(mfunc_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Mask / atlas / meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load atlas\n",
    "from nilearn import datasets\n",
    "atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr50-2mm')\n",
    "atlas_filename = atlas.maps\n",
    "\n",
    "# list of labels\n",
    "labels = atlas.labels\n",
    "\n",
    "print(labels)\n",
    "\n",
    "# number of regions\n",
    "number_of_regions = len(labels[1:])\n",
    "print(number_of_regions)\n",
    "\n",
    "# plot the atlas\n",
    "from nilearn import plotting\n",
    "plotting.plot_roi(atlas_filename, title=\"Harvard Oxford atlas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extract timecourses per ROI / block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract time series from ROIs\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "\n",
    "masker = NiftiLabelsMasker(labels_img=atlas_filename, standardize=True, memory='nilearn_cache')\n",
    "\n",
    "# time_series = masker.fit_transform(func_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir_events = [file for file in os.listdir(funcdir_path) if file.endswith('.tsv')]\n",
    "\n",
    "f_events = [os.path.join(funcdir_path, f) for f in fdir_events]\n",
    "\n",
    "f_events.sort()\n",
    "\n",
    "print(f_events)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Compute connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.image import index_img \n",
    "\n",
    "\n",
    "onset_set = {}\n",
    "duration_set = {}\n",
    "\n",
    "target_set = {}\n",
    "\n",
    "group_set = {}\n",
    "\n",
    "\n",
    "offset_st = 4 # offset for the onset time\n",
    "offset_end = 2 # offset for the end block time\n",
    "\n",
    "# Compute the correlation matrix and store it in the partial correlation matrix as a 4D array\n",
    "conn_measure = connectome.ConnectivityMeasure(kind='correlation',\n",
    "    standardize=\"zscore_sample\",)\n",
    "\n",
    "# intialize 2D array to store the partial correlation matrix \n",
    "partial_correlation_matrix = {}\n",
    "\n",
    "run_id = 0\n",
    "idx = 0\n",
    "\n",
    "# for each events file \n",
    "for fn in f_events:\n",
    "\n",
    "    print(os.path.join(funcdir_path,func_runs[run_id]), fn)\n",
    "\n",
    "    # Load the events.tsv file\n",
    "    events = pd.read_table(fn)\n",
    "\n",
    "    # Load the corresponding functional image\n",
    "    func_image = image.load_img(os.path.join(funcdir_path,func_runs[run_id]))\n",
    "\n",
    "    # extract time course from functional image\n",
    "    time_series = masker.fit_transform(func_image)\n",
    "\n",
    "    # create one image per event \n",
    "    for i, row in events.iterrows():\n",
    "\n",
    "        # Print the row information\n",
    "        # print(row) \n",
    "\n",
    "        # Extract the onset time\n",
    "        onset = np.round(row['onset'])\n",
    "\n",
    "        # Extract the duration\n",
    "        duration = np.round(row['duration'])\n",
    "\n",
    "        # Extract the trial_type\n",
    "        trial_type = row['trial_type']\n",
    "\n",
    "        # Print the row information\n",
    "        print(onset, duration, trial_type) \n",
    "\n",
    "        # if duration greater than 10 and not 'Noise'\n",
    "        if duration > 10 and onset + duration + offset_end < 660 and trial_type != 'Noise':\n",
    "\n",
    "            onset_set[idx] = onset + offset_st\n",
    "\n",
    "            duration_set[idx] = duration - offset_st + offset_end\n",
    "\n",
    "            target_set[idx] = trial_type\n",
    "\n",
    "            group_set[idx] = run_id\n",
    "\n",
    "            # Extract the time series for the event\n",
    "            event_time_series = time_series[int(onset):int(onset + duration), :]\n",
    "\n",
    "            \n",
    "            # 2D array for the correlation matrix\n",
    "            temp_correlation_matrix = conn_measure.fit_transform([event_time_series])[0]\n",
    "\n",
    "            # vectorize the correlation matrix\n",
    "            partial_correlation_matrix[idx] = temp_correlation_matrix.flatten()\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "    run_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Visualize average results per emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the average correlation matrix for each type of event\n",
    "\n",
    "for target in set(target_set.values()):\n",
    "    \n",
    "    # print(target)\n",
    "\n",
    "    # Normalize the correlation matrix\n",
    "\n",
    "    # Get the number of events for the target\n",
    "    counter = 0\n",
    "\n",
    "    fl_correlation_matrix = np.zeros((number_of_regions * number_of_regions))\n",
    "\n",
    "    for key, value in target_set.items():\n",
    "\n",
    "\n",
    "        if value == target:\n",
    "            fl_correlation_matrix += partial_correlation_matrix[key]\n",
    "            counter += 1\n",
    "\n",
    "\n",
    "    fl_correlation_matrix /= counter\n",
    "\n",
    "    TwoD_correlation_matrix = fl_correlation_matrix.reshape(number_of_regions,number_of_regions)\n",
    "    # Plot the correlation matrix\n",
    "    plotting.plot_matrix(TwoD_correlation_matrix, labels=labels[1:], vmax=0.8, vmin=-0.8, title=target)\n",
    "\n",
    "\n",
    "    # Save the correlation matrix as a nifti file\n",
    "    correlation_matrix_img = masker.inverse_transform(TwoD_correlation_matrix)\n",
    "\n",
    "    correlation_matrix_img.to_filename(f'correlation_matrix_{target}.nii.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare emotions and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# two-factorial anova analysis\n",
    "# one factor is the type of event\n",
    "# the other factor is the region\n",
    "\n",
    "# create a dataframe to store the correlation matrix\n",
    "df = pd.DataFrame(partial_correlation_matrix.values())\n",
    "\n",
    "# add the target column\n",
    "df['target'] = target_set.values()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns\n",
    "df.columns = [f'conn_{i}' for i in range(number_of_regions * number_of_regions)] + ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA analysis for each region\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "# create a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "df = df.rename(columns={0: \"conn0\"})\n",
    "\n",
    "for i in range(number_of_regions * number_of_regions):\n",
    "    \n",
    "        # create the formula\n",
    "        formula = f'conn_{i} ~ C(target)'\n",
    "    \n",
    "        # create the model\n",
    "        model = ols(formula, data=df).fit()\n",
    "    \n",
    "        # perform the ANOVA\n",
    "        aov_table = anova_lm(model, typ=2)\n",
    "    \n",
    "        # store the results\n",
    "        results[i] = aov_table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the p-value results into a square matrix\n",
    "anova_matrix = np.zeros((number_of_regions, number_of_regions))\n",
    "\n",
    "for i in range(number_of_regions):\n",
    "    for j in range(number_of_regions):\n",
    "        anova_matrix[i,j] = results[i * number_of_regions + j]['PR(>F)'][0]\n",
    "\n",
    "# Plot the p-values\n",
    "plotting.plot_matrix(anova_matrix, labels=labels[1:], vmax=0.05, vmin=0, title='ANOVA p-values', cmap='hot')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "\n",
    "# select feature with p-value < 0.05\n",
    "selected_features = np.where(anova_matrix.flatten() < 0.05)\n",
    "\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.maskers import NiftiMasker\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "from nilearn.decoding import Decoder\n",
    "# “background”: Use this option if your images present a clear homogeneous background.\n",
    "# “whole-brain-template”: This will extract the whole-brain part of your data by resampling \n",
    "# the MNI152 brain mask for your data’s field of view.\n",
    "\n",
    "\n",
    "logo = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_func_runs = [os.path.join(funcdir_path, f) for f in func_runs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking the data\n",
    "\n",
    "from nilearn.masking import compute_epi_mask, compute_multi_epi_mask\n",
    "\n",
    "mask_img = compute_multi_epi_mask(c_func_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mask image\n",
    "plot_img(mask_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input matrix\n",
    "X = np.array(list(partial_correlation_matrix.values()))\n",
    "\n",
    "# Select the features\n",
    "X = X[:, selected_features[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(target_set.values())\n",
    "\n",
    "# transform the target_set unique strings to integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# 0 agaisnt all others\n",
    "# y = np.where(y == 1, 0, 1)\n",
    "\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_group = list(group_set.values())\n",
    "\n",
    "print(runs_group)\n",
    "\n",
    "# unique values in runs_group\n",
    "print(np.unique(runs_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# number of unique in runs_group\n",
    "n_splits = len(np.unique(runs_group))\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def roc_auc_score_multiclass(actual_class, pred_class, average, multi_class):\n",
    "\n",
    "  #creating a set of all the unique classes using the actual class list\n",
    "  unique_class = set(actual_class)\n",
    "  roc_auc_dict = {}\n",
    "  for per_class in unique_class:\n",
    "    #creating a list of all the classes except the current class \n",
    "    other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "    #marking the current class as 1 and all other classes as 0\n",
    "    new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "    new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "    #using the sklearn metrics method to calculate the roc_auc_score\n",
    "    roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average, multi_class = multi_class)\n",
    "    roc_auc_dict[per_class] = roc_auc\n",
    "\n",
    "  return roc_auc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
    "\n",
    "accuracy = {}\n",
    "lr_roc_auc_multiclass  = {}\n",
    "roc_vals_mc = {}\n",
    "cm = {}\n",
    "\n",
    "f = 0\n",
    "\n",
    "for train, test in logo.split(X, y, groups=runs_group): \n",
    "    # print(\"%s %s\" % (train, test))\n",
    "    X_train, X_test = X[train], X[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "\n",
    "    # print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # predict the labels\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_prob = clf.predict_proba(X_test)\n",
    "\n",
    "    # compute the accuracy\n",
    "    accuracy[f] = np.mean(y_pred == y_test)\n",
    "\n",
    "    # compute the confusion matrix\n",
    "    cm[f] = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # compute the ROC\n",
    "    roc_vals = roc_auc_score_multiclass(y_test, y_pred, average = 'micro', multi_class = 'ovr')\n",
    "\n",
    "\n",
    "    lr_roc_auc_multiclass[f] = list(roc_vals.values())\n",
    "\n",
    "\n",
    "    roc_vals_mc[f] = roc_auc_score(y_test, y_pred_prob, multi_class='ovr', average='weighted')\n",
    "\n",
    "    f += 1\n",
    "    \n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'ROC AUC: {lr_roc_auc_multiclass}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_vals_mc\n",
    "\n",
    "print(np.mean(list(roc_vals_mc.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean of the ROC AU\n",
    "mean_roc_auc = np.mean(list(lr_roc_auc_multiclass.values()), axis=1)\n",
    "\n",
    "print(f'Mean ROC AUC: {mean_roc_auc}')\n",
    "\n",
    "\n",
    "\n",
    "# compute mean of the accuracy\n",
    "mean_accuracy = (list(accuracy.values()))\n",
    "\n",
    "print(f'Mean accuracy: {mean_accuracy}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# plot confusion matrix totals\n",
    "cm_total = np.zeros(cm[0].shape)\n",
    "\n",
    "for i in range(len(cm)):\n",
    "    cm_total += cm[i]\n",
    "\n",
    "\n",
    "\n",
    "sns.heatmap(cm_total, annot= True, fmt='g', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_, )\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, tree\n",
    "\n",
    "#clf = svm.SVC(kernel='linear', C=1)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
    "\n",
    "\n",
    "# create cross validation using logo\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=logo, groups=runs_group,     scoring='roc_auc_ovr')\n",
    "\n",
    "print(scores)\n",
    "\n",
    "scores = cross_val_score(clf, X, y, cv=logo, groups=runs_group,     scoring='accuracy')\n",
    "\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 visualization of connectivity among classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizer the distribution of the scores\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.boxplot(x=y, y = X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvpa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
