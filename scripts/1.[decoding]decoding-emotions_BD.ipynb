{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music and the Brain \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and load data from 0.[decoding]feature-extraction notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "\n",
    "import nilearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nibabel as nib\n",
    "\n",
    "from nilearn import plotting\n",
    "from nilearn import image\n",
    "from nilearn.plotting import plot_anat, plot_img, plot_stat_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the data directory\n",
    "src_dir = '../data/derivatives/nilearn'\n",
    "\n",
    "# Set the participant and session IDs\n",
    "part_id = 'sub-02'\n",
    "ses_id = 'ses-01'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(src_dir, part_id, ses_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature set.\n",
    "X = image.load_img(os.path.join(data_dir, f'{part_id}_{ses_id}_task-02a-MVPA-12sBOLD.nii.gz'))\n",
    "\n",
    "# Load csv file with targets.\n",
    "target_set = pd.read_csv(os.path.join(data_dir, f'{part_id}_{ses_id}_task-02a-MVPA-12sBOLD_targets.csv'))\n",
    "\n",
    "# Load csv file with groups.\n",
    "group_set = pd.read_csv(os.path.join(data_dir, f'{part_id}_{ses_id}_task-02a-MVPA-12sBOLD_groups.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mask.\n",
    "mask_img = image.load_img(os.path.join(data_dir, f'{part_id}_{ses_id}_task-02a-MVPA_multi-epi-mask.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.maskers import NiftiMasker\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "from nilearn.decoding import Decoder\n",
    "# “background”: Use this option if your images present a clear homogeneous background.\n",
    "# “whole-brain-template”: This will extract the whole-brain part of your data by resampling \n",
    "# the MNI152 brain mask for your data’s field of view.\n",
    "\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72, 2304)\n"
     ]
    }
   ],
   "source": [
    "# concat images in list\n",
    "# X = nilearn.image.concat_imgs(list(image_set.values()))\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Create target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 5 5 7 7 0 0 3 3 1 1 6 6 4 4 8 8 0 0 7 7 1 1 2 2 3 3 4 4 8 8 6 6 5 5 7\n",
      " 7 4 4 1 1 8 8 2 2 5 5 0 0 6 6 3 3 2 2 5 5 1 1 6 6 4 4 3 3 7 7 0 0 8 8]\n"
     ]
    }
   ],
   "source": [
    "y = list(target_set['target'].values)\n",
    "\n",
    "# transform the target_set unique strings to integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# 0 agaisnt all others\n",
    "# y = np.where(y == 1, 0, 1)\n",
    "\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "runs_group = group_set['group'].values\n",
    "\n",
    "print(runs_group)\n",
    "\n",
    "# unique values in runs_group\n",
    "print(np.unique(runs_group))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Nilearn wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mask_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m scoring_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(\n\u001b[1;32m      4\u001b[0m     estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvc\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      5\u001b[0m     cv \u001b[38;5;241m=\u001b[39m logo, \n\u001b[1;32m      6\u001b[0m     smoothing_fwhm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m,  \n\u001b[0;32m----> 7\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask_img\u001b[49m,\n\u001b[1;32m      8\u001b[0m     standardize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     scoring\u001b[38;5;241m=\u001b[39mscoring_method,\n\u001b[1;32m     10\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mask_img' is not defined"
     ]
    }
   ],
   "source": [
    "scoring_method = 'roc_auc'\n",
    "\n",
    "decoder = Decoder(\n",
    "    estimator='svc', \n",
    "    cv = logo, \n",
    "    smoothing_fwhm = 3,  \n",
    "    mask = mask_img,\n",
    "    standardize=True,\n",
    "    scoring=scoring_method,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.1 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.fit(\n",
    "    X, \n",
    "    y, \n",
    "    groups = runs_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.2 Cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get values of decoder_LogisticL1.cv_scores_ per fold and create list\n",
    "cv_scores = [decoder.cv_scores_[i] for i in range(len(decoder.cv_scores_))]\n",
    "\n",
    "# mean of cv_scores per class\n",
    "cv_scores_mean = np.mean(cv_scores, axis=1)\n",
    "\n",
    "# create pandas dataframe with cv_scores_mean and labels\n",
    "df = pd.DataFrame(cv_scores_mean, columns=['score'])\n",
    "df['label'] = le.classes_\n",
    "\n",
    "# print the dataframe\n",
    "print(df)\n",
    "\n",
    "print(np.mean(cv_scores_mean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results cv_scores_mean into a csv file with columns: target, cv_score\n",
    "results_dir = '../results'\n",
    "if not os.path.exists(os.path.join(results_dir, 'nilearn', part_id)):\n",
    "    os.makedirs(os.path.join(results_dir, 'nilearn', part_id))\n",
    "\n",
    "fn = f'{part_id}_{ses_id}_task-02a_MVPA12sBOLD_cv-scores.csv'\n",
    "\n",
    "df = pd.DataFrame({'target': le.classes_, 'cv_score': cv_scores_mean, 'classifier': 'svc_nilearn_wrapper', 'scoring': scoring_method})\n",
    "\n",
    "# read the csv file and append the new data\n",
    "if os.path.exists(os.path.join(results_dir, 'nilearn', part_id, fn)):\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(results_dir, 'nilearn', part_id, fn))\n",
    "\n",
    "    # add new data to df\n",
    "    df = pd.concat([df, pd.DataFrame({'target': le.classes_, 'cv_score': cv_scores_mean, 'classifier': 'svc_nilearn_wrapper', 'scoring': scoring_method})])\n",
    "\n",
    "df.to_csv(os.path.join(results_dir, 'nilearn', part_id, fn), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Dummy predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize y in sets of unique runs_groups\n",
    "y_random = np.array(y)\n",
    "for i in np.unique(runs_group):\n",
    "    y_random[runs_group == i] = np.random.permutation(y_random[runs_group == i])\n",
    "\n",
    "\n",
    "decoder_random = Decoder(\n",
    "    estimator='svc', \n",
    "    cv = logo, \n",
    "    smoothing_fwhm = 3,  \n",
    "    mask = mask_img,\n",
    "    standardize=\"zscore_sample\")\n",
    "\n",
    "decoder_random.fit(\n",
    "    X, \n",
    "    y_random, \n",
    "    groups = runs_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get values of decoder_LogisticL1.cv_scores_ per fold and create list\n",
    "cv_scores_random = [decoder_random.cv_scores_[i] for i in range(len(decoder_random.cv_scores_))]\n",
    "\n",
    "# mean of cv_scores per class\n",
    "cv_scores_mean_random = np.mean(cv_scores_random, axis=1)\n",
    "\n",
    "print(cv_scores_mean_random)\n",
    "\n",
    "print(np.mean(cv_scores_mean_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dummy = Decoder(\n",
    "    estimator='dummy_classifier', \n",
    "    cv = logo, \n",
    "    smoothing_fwhm = 3,  \n",
    "    mask = mask_img,\n",
    "    standardize=\"zscore_sample\")\n",
    "\n",
    "decoder_dummy.fit(\n",
    "    X, \n",
    "    y, \n",
    "    groups = runs_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get values of decoder_LogisticL1.cv_scores_ per fold and create list\n",
    "cv_scores_dummy = [decoder_dummy.cv_scores_[i] for i in range(len(decoder_dummy.cv_scores_))]\n",
    "\n",
    "# mean of cv_scores per class\n",
    "cv_scores_mean_dummy = np.mean(cv_scores_dummy, axis=1)\n",
    "\n",
    "print(cv_scores_mean_dummy)\n",
    "\n",
    "print(np.mean(cv_scores_mean_dummy)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Plotting weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot for each emotion\n",
    "\n",
    "for i in range(len(np.unique(y))):\n",
    "\n",
    "    # get the indices of the target_set\n",
    "    idx = np.where(y == i)\n",
    "\n",
    "    # plot the cv_scores_mean_i\n",
    "    plt = plotting.plot_stat_map(\n",
    "        decoder.coef_img_[i],\n",
    "        title=f'{le.inverse_transform([i])[0]}',\n",
    "        dim=-1,\n",
    "        threshold=\"auto\",\n",
    "        cut_coords=(0, 0, 0),\n",
    "        draw_cross=False,\n",
    "    )\n",
    "\n",
    "    # save the plot\n",
    "    # plt.savefig(os.path.join(results_dir, f'{part_id}_{ses_id}_task-02a_MVPA_12sBOLD_{le.inverse_transform([i])[0]}.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Weights across folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape\n",
    "list(decoder.coef_img_.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crete mean image of the images in dict decoder.coef_img_\n",
    "from nilearn.image import mean_img\n",
    "\n",
    "\n",
    "\n",
    "# compute sum of all images in list\n",
    "\n",
    "# for each image in the list extract the data and sum them\n",
    "imgs = decoder.coef_img_.values()\n",
    "\n",
    "# create list img_data\n",
    "img_data = []\n",
    "\n",
    "i = 0\n",
    "for img in imgs:\n",
    "    # compute the absolute value of the image data\n",
    "    img_data.append(np.abs(img.get_fdata()))\n",
    "\n",
    "    i += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# mean_img_coef = mean_img(list(decoder.coef_img_.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = np.array(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of the images\n",
    "mean_img_coef = np.sum(img_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create image from mean_img_coef\n",
    "mean_img_coef = nib.Nifti1Image(mean_img_coef, mask_img.affine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the cv_scores_mean_i\n",
    "plotting.view_img(\n",
    "    mean_img_coef,\n",
    "    title='mean image',\n",
    "    threshold=\"99%\",\n",
    "    cut_coords=(0, 0, 0),\n",
    "    draw_cross=True,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification using scikit routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.maskers import NiftiMasker\n",
    "\n",
    "# Extract data using masker\n",
    "\n",
    "\n",
    "masker = NiftiMasker(mask_img = mask_img, \n",
    "                    smoothing_fwhm=3,\n",
    "                    standardize=\"zscore_sample\",)\n",
    "\n",
    "X_data = masker.fit_transform(X)\n",
    "\n",
    "\n",
    "print('shape of X:', X_data.shape)\n",
    "\n",
    "# feature selection based on anova\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# create an instance of SelectKBest\n",
    "selector = SelectKBest(f_classif, k=52000)\n",
    "\n",
    "# fit the selector to the data\n",
    "selector.fit(X_data, y)\n",
    "\n",
    "# get the indices of the selected features\n",
    "idx = selector.get_support(indices=True)\n",
    "\n",
    "# get the scores of the selected features\n",
    "scores = selector.scores_[idx]\n",
    "\n",
    "# get the p-values of the selected features\n",
    "pvalues = selector.pvalues_[idx]\n",
    "\n",
    "# get the selected features\n",
    "X_data_selected = X_data[:, idx]\n",
    "\n",
    "print('shape of X_selected:', X_data_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of unique in runs_group\n",
    "n_splits = len(np.unique(runs_group))\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "#for train, test in logo.split(X_data, y, groups=runs_group):\n",
    "#   print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def roc_auc_score_multiclass(actual_class, pred_class, average, multi_class):\n",
    "\n",
    "  #creating a set of all the unique classes using the actual class list\n",
    "  unique_class = set(actual_class)\n",
    "  roc_auc_dict = {}\n",
    "  for per_class in unique_class:\n",
    "    #creating a list of all the classes except the current class \n",
    "    other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "    #marking the current class as 1 and all other classes as 0\n",
    "    new_actual_class = [0 if x in other_class else 1 for x in actual_class]\n",
    "    new_pred_class = [0 if x in other_class else 1 for x in pred_class]\n",
    "\n",
    "    #using the sklearn metrics method to calculate the roc_auc_score\n",
    "    roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = average, multi_class = multi_class)\n",
    "    roc_auc_dict[per_class] = roc_auc\n",
    "\n",
    "  return roc_auc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "clf = svm.SVC()\n",
    "\n",
    "accuracy = {}\n",
    "lr_roc_auc_multiclass  = {}\n",
    "roc_vals_mc = {}\n",
    "cm = {}\n",
    "\n",
    "f = 0\n",
    "\n",
    "for train, test in logo.split(X_data_selected, y, groups=runs_group): \n",
    "    # print(\"%s %s\" % (train, test))\n",
    "    X_train, X_test = X_data_selected[train], X_data_selected[test]\n",
    "    y_train, y_test = y[train], y[test]\n",
    "\n",
    "    # print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # predict the labels\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # y_pred_prob = clf.predict_proba(X_test)\n",
    "\n",
    "    # compute the confusion matrix\n",
    "    cm[f] = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # We will store the results in a dictionary for easy access later\n",
    "    per_class_accuracies = {}\n",
    "\n",
    "    # Calculate the accuracy for each one of our classes\n",
    "    for idx, cls in enumerate(set(y_test)):\n",
    "        # True negatives are all the samples that are not our current GT class (not the current row) and were not predicted as the current class (not the current column)\n",
    "        true_negatives = np.sum(np.delete(np.delete(cm[f], idx, axis=0), idx, axis=1))\n",
    "        \n",
    "        # True positives are all the samples of our current GT class that were predicted as such\n",
    "        true_positives = cm[f][idx, idx]\n",
    "        \n",
    "        # The accuracy for the current class is the ratio between correct predictions to all predictions\n",
    "        per_class_accuracies[cls] = (true_positives + true_negatives) / np.sum(cm[f])\n",
    "\n",
    "    accuracy[f] = per_class_accuracies\n",
    "\n",
    "\n",
    "    # compute the ROC\n",
    "    roc_vals = roc_auc_score_multiclass(y_test, y_pred, average = 'micro', multi_class = 'ovr')\n",
    "\n",
    "\n",
    "    lr_roc_auc_multiclass[f] = list(roc_vals.values())\n",
    "\n",
    "\n",
    "    # roc_vals_mc[f] = roc_auc_score(y_test, y_pred_prob, multi_class='ovr', average='weighted')\n",
    "\n",
    "    f += 1\n",
    "    \n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'ROC AUC: {lr_roc_auc_multiclass}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get values of accuracy per fold and create list\n",
    "cv_accuracy = [list(accuracy[i].values()) for i in range(len(accuracy))]\n",
    "\n",
    "\n",
    "print(cv_accuracy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean of the ROC AU\n",
    "mean_roc_auc = np.mean(list(lr_roc_auc_multiclass.values()), axis=0)\n",
    "\n",
    "print(f'Mean ROC AUC: {mean_roc_auc}')\n",
    "\n",
    "\n",
    "\n",
    "# compute mean of the accuracy\n",
    "mean_accuracy = np.mean(cv_accuracy, axis=0)\n",
    "\n",
    "print(f'Mean accuracy: {mean_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# plot confusion matrix totals\n",
    "cm_total = np.zeros(cm[0].shape)\n",
    "\n",
    "for i in range(len(cm)):\n",
    "    cm_total += cm[i]\n",
    "\n",
    "\n",
    "\n",
    "sns.heatmap(cm_total, annot= True, fmt='g', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_, )\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvpa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
